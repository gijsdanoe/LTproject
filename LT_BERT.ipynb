{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eEfSrg05uXY",
        "outputId": "1138ef22-22ca-4fe6-f319-558f5eca87fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUE6pVwa1V_D",
        "outputId": "6b7aaea6-8876-4148-cda2-8ea77ceb7737",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8c2ac68fef25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import csv\n",
        "from transformers import AutoTokenizer,AutoModelForSequenceClassification,TrainingArguments,Trainer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import torch\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from collections import defaultdict\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRksnIPs3hXK"
      },
      "outputs": [],
      "source": [
        "train_arguments = []\n",
        "train_frames = []\n",
        "train_topics = []\n",
        "with open('/content/drive/MyDrive/LT_project_data/Train.csv') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "        train_arguments.append(row[2])\n",
        "        train_frames.append(row[-1])\n",
        "        train_topics.append(row[4])\n",
        "\n",
        "val_arguments = []\n",
        "val_frames = []\n",
        "val_topics = []\n",
        "with open('/content/drive/MyDrive/LT_project_data/Validation.csv') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "        val_arguments.append(row[2])\n",
        "        val_frames.append(row[-1])\n",
        "        val_topics.append(row[4])\n",
        "\n",
        "test_arguments = []\n",
        "test_frames = []\n",
        "test_topics = []\n",
        "with open('/content/drive/MyDrive/LT_project_data/Test.csv') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "        test_arguments.append(row[2])\n",
        "        test_frames.append(row[-1])\n",
        "        test_topics.append(row[4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A53vkQcnRjb5"
      },
      "outputs": [],
      "source": [
        "# # REMOVE 'Other' and 'Irrelevant' classes\n",
        "\n",
        "train = list(zip(train_arguments, train_frames,train_topics))\n",
        "val = list(zip(val_arguments, val_frames,val_topics))\n",
        "test = list(zip(test_arguments, test_frames,test_topics))\n",
        "\n",
        "train = [s for s in train if s[1] not in ['Other','Irrelevant']]\n",
        "val = [s for s in val if s[1] not in ['Other','Irrelevant']]\n",
        "test = [s for s in test if s[1] not in ['Other','Irrelevant']]\n",
        "\n",
        "train_arguments = list(list(zip(*train))[0])\n",
        "train_frames = list(list(zip(*train))[1])\n",
        "train_topics = list(list(zip(*train))[2])\n",
        "\n",
        "val_arguments = list(list(zip(*val))[0])\n",
        "val_frames = list(list(zip(*val))[1])\n",
        "val_topics = list(list(zip(*val))[2])\n",
        "\n",
        "test_arguments = list(list(zip(*test))[0])\n",
        "test_frames = list(list(zip(*test))[1])\n",
        "test_topics = list(list(zip(*test))[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8q6jo6ndLzV",
        "outputId": "db39de2d-98ee-4fd5-ff9f-aa1bfbb1a9d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# # REMOVE IMPORTANT FEATURES PER TOPIC\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(train_arguments)\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(train_topics)\n",
        "Y = le.transform(train_topics)\n",
        "\n",
        "clf = SGDClassifier(loss='log')\n",
        "clf.fit(X, Y)\n",
        "\n",
        "labeldict = defaultdict(list)\n",
        "for i in range(0, clf.coef_.shape[0]):\n",
        "    top20_indices = np.argsort(clf.coef_[i])[-50:]\n",
        "    for j in top20_indices:\n",
        "      labeldict[le.classes_[i]].append(vectorizer.get_feature_names()[j])\n",
        "\n",
        "train = list(zip(train_arguments, train_topics))\n",
        "val = list(zip(val_arguments, val_topics))\n",
        "test = list(zip(test_arguments, test_topics))\n",
        "\n",
        "train_arguments = []\n",
        "for i in train:\n",
        "  sent = i[0].split()\n",
        "  for word in sent:\n",
        "    if word in labeldict[i[1]]:\n",
        "      sent[sent.index(word)] = '[MASK]'\n",
        "  train_arguments.append(' '.join(sent))\n",
        "\n",
        "val_arguments = []\n",
        "for i in val:\n",
        "  sent = i[0].split()\n",
        "  for word in sent:\n",
        "    if word in labeldict[i[1]]:\n",
        "      sent[sent.index(word)] = '[MASK]'\n",
        "  val_arguments.append(' '.join(sent))\n",
        "\n",
        "test_arguments = []\n",
        "for i in test:\n",
        "  sent = i[0].split()\n",
        "  for word in sent:\n",
        "    if word in labeldict[i[1]]:\n",
        "      sent[sent.index(word)] = '[MASK]'\n",
        "  test_arguments.append(' '.join(sent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgWhu4RO5LrW"
      },
      "outputs": [],
      "source": [
        "labeldict = {'Morality':0,'Quality of Life':1,'Crime and punishment':2,'International relations and reputation':3,'Fairness and equality':4,'Cultural identity':5,'Political':6,'Capacity and resources':7,'Security and defense':8,'Health and Safety':9,'Economic':10,'Climate and environment':11,'Historical':12,'Policy prescription and evaluation':13,'Education':14,'Technology and innovation':15,'Legality, constitutionality and jurisprudence':16,'Public opinion':17, 'Irrelevant':18, 'Other':19}\n",
        "train_frames_bin = []\n",
        "val_frames_bin = []\n",
        "test_frames_bin = []\n",
        "for label in train_frames:\n",
        "    train_frames_bin.append(labeldict[label])\n",
        "for label in val_frames:\n",
        "    val_frames_bin.append(labeldict[label])\n",
        "for label in test_frames:\n",
        "    test_frames_bin.append(labeldict[label])\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "train_encodings = tokenizer(train_arguments, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_arguments, truncation=True, padding=True)\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = Dataset(train_encodings, train_frames_bin)\n",
        "val_dataset = Dataset(val_encodings, val_frames_bin)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "UctJjRKlIKG_",
        "outputId": "e7e2a751-7da0-40da-d369-be5cd10d7145"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1305\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 492\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='492' max='492' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [492/492 06:20, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.938200</td>\n",
              "      <td>2.918811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.856400</td>\n",
              "      <td>2.862777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.783000</td>\n",
              "      <td>2.817663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.686600</td>\n",
              "      <td>2.787793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.584300</td>\n",
              "      <td>2.739985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.344400</td>\n",
              "      <td>2.541488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.181500</td>\n",
              "      <td>2.448352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.999200</td>\n",
              "      <td>2.515989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.910800</td>\n",
              "      <td>2.369000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 152\n",
            "  Batch size = 20\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 152\n",
            "  Batch size = 20\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 152\n",
            "  Batch size = 20\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 152\n",
            "  Batch size = 20\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 152\n",
            "  Batch size = 20\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 152\n",
            "  Batch size = 20\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 152\n",
            "  Batch size = 20\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 152\n",
            "  Batch size = 20\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 152\n",
            "  Batch size = 20\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=492, training_loss=2.4294472826205618, metrics={'train_runtime': 380.7551, 'train_samples_per_second': 10.282, 'train_steps_per_second': 1.292, 'total_flos': 899437126813020.0, 'train_loss': 2.4294472826205618, 'epoch': 3.0})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "max_length = 512\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=18).to('cuda')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = './results' ,          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
        "    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
        "    logging_steps=50,               # log & save weights each logging_steps\n",
        "    evaluation_strategy=\"steps\",                 # evaluate each `logging_steps`\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset, put in train_datasetQ1 or train_datasetQ2\n",
        "    eval_dataset=val_dataset           # evaluation dataset, put in valid_datasetQ1 or valid_datasetQ2\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3v3QSoJMl5Rq",
        "outputId": "f4eaf1bf-316d-4c81-a884-6d97855eae37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      1.000     0.125     0.222         8\n",
            "           1      0.182     0.364     0.242        11\n",
            "           2      1.000     0.000     0.000         4\n",
            "           3      1.000     0.000     0.000         1\n",
            "           4      0.143     0.059     0.083        17\n",
            "           5      0.188     0.600     0.286         5\n",
            "           6      0.333     0.714     0.455         7\n",
            "           7      1.000     0.000     0.000         9\n",
            "           8      0.000     1.000     0.000         0\n",
            "           9      0.000     0.000     0.000         1\n",
            "          10      0.500     0.125     0.200         8\n",
            "          11      1.000     0.020     0.040        49\n",
            "          12      0.385     0.417     0.400        12\n",
            "          13      1.000     0.059     0.111        17\n",
            "          15      0.062     0.750     0.115         4\n",
            "          16      0.783     0.562     0.655        32\n",
            "          17      1.000     0.000     0.000         5\n",
            "\n",
            "    accuracy                          0.226       190\n",
            "   macro avg      0.563     0.282     0.165       190\n",
            "weighted avg      0.708     0.226     0.222       190\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def get_prediction(text):\n",
        "    # prepare our text into tokenized sequence\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
        "    # perform inference to our model\n",
        "    outputs = model(**inputs)\n",
        "    # get output probabilities by doing softmax\n",
        "    probs = outputs[0].softmax(1)\n",
        "    # executing argmax function to get the candidate label\n",
        "    return probs.argmax()\n",
        "\n",
        "preds = []\n",
        "for x in test_arguments:\n",
        "  preds.append(get_prediction(x).tolist())\n",
        "\n",
        "print(classification_report(test_frames_bin, preds, zero_division=True, digits=3))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LT_BERT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}